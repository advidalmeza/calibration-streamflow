---
title: 'Assignment 5: Model Performance Evaluation'
author: 'Lewis White, Jessica French, Alessandra Vidal Meza'
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load packages
library(here)
library(sensitivity)
library(tidyverse)
library(lubridate)
library(reldist)
library(purrr)
library(ggpubr)

# Source NSE calculating function 
source(here::here("R", "nse.R"))

# Source relative error calculating function
source(here::here("R", "relerr.R"))

# Source combined performance calculating function for NSE and relative error with 50/50 weighting 
source(here::here("R", "cper.R"))

sager <- read.table(here::here('Data', 'sager.txt'), header = T)
msage <- read.table(here::here('Data', 'sagerm.txt'), header = T)

```

```{r include = FALSE}
sager <- sager %>% 
  mutate(date = paste(day, month, year, sep = '/')) %>%
  mutate(date = as.Date(date,'%d/%m/%Y'))

# pivot longer
sagerl <- sager %>% 
  pivot_longer(cols = c("model","obs"),
               names_to = "source",
               values_to = "flow")

msagel <-  msage %>% 
  pivot_longer(cols = !c(date, month, year, day, wy, obs), 
               names_to = 'sim', values_to = 'flow')
```

# Scale and Subsetting Data

```{r}
# Set different time step
sager_wy <- sager %>% 
  group_by(wy) %>% 
  summarize(model = sum(model), obs = sum(obs))

# Sum by month
tmp <- sager %>% 
  group_by(wy, month) %>% 
  summarize(model = sum(model), obs = sum(obs))
```

```{r}
# Extract for winter months
sager_winter <- subset(tmp, month %in% c(12, 1, 2))

# Find correlation for winter
cor(sager_winter$model, sager_winter$obs)

# Find Nash Sutcliffe Efficiency for winter
nse(m = sager_winter$model, o = sager_winter$obs)

# Find relative error for winter
relerr(m = sager_winter$model, o = sager_winter$obs)*100

# Find combined performance for winter
cper(m = sager_winter$model, o = sager_winter$obs, weight.nse = 0.8)
```


```{r}


# subset only acceptable runs
msagel_acc <- subset(msagel, sim %in% res_acc$sim)
# join with weights from res_acc, left_join will repeat weights for each day in streamflow trajectory
msagel_acc <- left_join(msagel_acc, res_acc, by = 'sim')


# finally multiply flow by weight
msagel_acc <- msagel_acc %>% 
  mutate(flow_wt = flow*wt_acc)

# now we can average streamflow for each day from all the runs # using the weights
aver_flow <- msagel_acc %>% 
  group_by(date) %>% 
  dplyr::summarize(meanstr = sum(flow_wt))

# add some date information or simply add to simQ

ggplot(aver_flow, aes(x=date, y=meanstr))+geom_line(col="red")+labs(y="Streamflow mm/day")

# add some of the other date info and plot a subset
aver_flow$wy = msage$wy
wycheck=1985
ggplot(subset(aver_flow, wy == wycheck), aes(x=date, y=meanstr, col="model_wt"))+
  geom_line()+labs(y="Streamflow mm/day")+
  geom_line(data=subset(msage, wy==wycheck), aes(date, obs, col="obs")) 
```


## Metric Function 

Let's start loading the highflowmetrics() function:
```{r}
source(here::here("R", "highflowmetrics.R"))
```

Explain why

## Calibration: Apply Metric Function 

```{r}
nse(m = sager$model, o = sager$obs)
relerr(m = sager$model, o = sager$obs)*100
cper(m = sager$model, o = sager$obs, weight.nse = 0.8)
```

```{r}
# add your code here (or start new file if you want)

res <- short_msage %>% 
  select(-date, -month, -day, -year, -wy, -obs) %>%
  map_df(compute_highflowmetrics, o = short_msage$obs,
         month = short_msage$month, day = short_msage$day, 
         year = short_msage$year, wy = short_msage$wy)

res$sim <- snames

# graph range of performance measures
resl <- res %>% 
  pivot_longer(-sim, names_to = "metric", values_to = "value")

# select the best one based on the combined metric
best <- res[which.max(res$combined),]

# running the model forward
# so we can look at the full time series
 
# for comparison lets consider how worst and best parameters perform for subsequent simulations
# focusing specifically on August streamflow
 worst <- res[which.min(res$combined),]
 
 compruns = msage %>%
   select(best$sim, worst$sim, date, obs, month, day, year, wy)

 compruns = subset(compruns, wy > 1970)
 
 compruns_mwy = compruns %>% 
   select(-c(day,date, year)) %>% 
   group_by(month, wy) %>% 
   summarize(across(everything(), mean))
 
 compruns_mwyl = compruns_mwy %>% 
   pivot_longer(cols = !c(month, wy), 
                names_to = "sim", 
                values_to = "flow")


```



  * Part 2 from above: 
    1. Apply your performance function to a subset of the Sagehen data set (with multiple simulations) that you want to use for calibration 
    2. Summarize the performance over the calibration period in 1-2 graphs; you can decide what is useful 

### Performance Evaluation

```{r}
ggplot(resl, aes(metric, value)) + 
  geom_boxplot() + 
  facet_wrap(~metric, scales = "free")
```

```{r}
# lets start with streamflow estimates from best performing parameter set
 ggplot(msage, aes(date, msage[,best$sim])) + 
   geom_line() + 
   geom_line(aes(date, obs), col = "red") 

```

```{r}
 compruns_mwyl %>% 
   subset(month == 4) %>% 
   ggplot(aes(sim, flow)) + 
   geom_boxplot()
```


## Discussion 
A short paragraph discussing why you choose the output and performance measures that you did and some thoughts (1-2 sentences) on what your calibration and post-calibration uncertainty analysis tells you


# Measure Performance using different metrics


# In Class Exercise

* Read in the *sager.txt* data

* Think of a new metric that might be interesting from a particular environmental context

* Code that metric as a function - and then apply it

# Using multiple metrics. 

* depends on what you want the model to get right

* type of data that you have for evaluation (its resolution and accuracy)

# Your turn! Part 1: Come up with a combined metric that you think is interesting 

* include at least one "sub" metric that needs to be transformed (for example, the `annual_min_err_trans` above)

* Be creative 
    * you can subset, aggregate, focus only on particular type of years or days
    * think about ecological or human water uses that depend on certain flow conditions

# Calibration

Calibration is picking parameter sets based on performance evaluation

Apply metrics over multiple outputs (generated by running across many parameters sets) - like we've done in our sensitivity analysis work

**Example** - a dataset where each column
is a different model run for Sagehen Creek
(using different parameters) - don't worry about the parameters for now

* sagerm.txt

**Split-sample**: split time period into 
  * calibration time period (used to pick parameter sets)
  * validation time period (used to see how well chose paramter sets perform)
  
  
**Thoughts**

In many cases - you just run calibration sample first - and then only run validation for parameters that you choose
here I ran for all parameters sets for the full time period so that we can explore

We could also envision this as a 'lab' where we only had a few years of observed streamflow data for calibration
and want to see going forward how much parameter selection influences results


Some code to help organize things

## i think this could be deleted 
```{r multiple}

# multiple results - lets say we've run the model for multiple years, 
#each column  is streamflow for a different parameter set

# keep track of number of simulations (e.g results for each parameter set) 
# use as a column names
nsim = ncol(msage)
snames = sprintf("S%d", seq(from = 1, to = nsim))
colnames(msage) = snames


# lets say we know the start date from our earlier output
msage$date = sager$date
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy


# lets add observed
msage <- left_join(msage, sager[,c("obs","date")], by = c("date"))

# how can we plot all results - lets plot water year 1970 otherwise its hard to see
msagel <- msage %>% 
  pivot_longer(cols = !c(date, month, year, day,wy), 
               names_to = "run", 
               values_to = "flow")

p1 <- ggplot(subset(msagel, wy == 1970), 
            aes(as.Date(date), flow, col = run)) + 
  geom_line() + 
  theme(legend.position = "none")

p1

# lets add observed streamflow
p1 + 
  geom_line(data = subset(sager, wy == 1970), 
            aes(as.Date(date), obs), 
            size = 2, col = "black", linetype = 2) + 
  labs(y = "Streamflow", x = "Date")

# subset for split sample calibration
short_msage <- subset(msage, wy < 1975)

# compute performance measures for output from all parameters
res <- short_msage %>% 
  select(!c("date","month","year","day","wy","obs")) %>%
  map_dbl(nse, short_msage$obs) #

# purrr function here! map_dbl will apply the function nse() to each column in our data frame against the observed and returns a vector

head(res)

# another example using our low flow statistics
# use apply to compute for all the data
source(here("R", "compute_lowflowmetrics_all.R"))

res <- short_msage %>% 
  select(-date, -month, -day, -year, -wy, -obs) %>%
  map_df(compute_lowflowmetrics_all, o = short_msage$obs, 
         month = short_msage$month, day = short_msage$day, 
         year = short_msage$year, wy = short_msage$wy)
# note here we use map_df to get a dataframe back 

# interesting to look at range of metrics - could use this to decide on
# acceptable values
summary(res)
# we can add a row that links with simulation number
res$sim = snames

# graph range of performance measures
resl <- res %>% 
  pivot_longer(-sim,
               names_to = "metric", 
               values_to = "value")

ggplot(resl, aes(metric, value)) + 
  geom_boxplot() + 
  facet_wrap(~metric, scales = "free")


# select the best one based on the combined metric
best = res[which.max(res$combined),]

# running the model forward
# so we can look at the full time series

# lets start with streamflow estimates from best performing parameter set
 ggplot(msage, aes(date, msage[,best$sim])) + 
   geom_line() + 
   geom_line(aes(date, obs), col = "red") 

 
# for comparison lets consider how worst and best parameters perform for subsequent simulations
# focusing specifically on August streamflow
 worst = res[which.min(res$combined),]
 
 compruns = msage %>%
   select(best$sim, worst$sim, date, obs, month, day, year, wy)

 compruns = subset(compruns, wy > 1970)
 
 compruns_mwy = compruns %>% 
   select(-c(day,date, year)) %>% 
   group_by(month, wy) %>% 
   summarize(across(everything(), mean))
 
 compruns_mwyl = compruns_mwy %>% 
   pivot_longer(cols = !c(month, wy), 
                names_to = "sim", 
                values_to = "flow")
 
 compruns_mwyl %>% 
   subset(month == 8) %>% 
   ggplot(aes(sim, flow)) + 
   geom_boxplot()
```

# Your turn! Part 2: Using your performance metric

* Perform a split-sample calibration - you can decide what year to use for
calibration (its an experiment!)

* Find the best parameter set, and then graph something about streamflow (e.g daily, mean August, or ?) for the best parameter set

* Compute how the performance of the model using the best parameter set changed
in pre and post calibration periods

On the canvas survey - add the 'best' parameter set number (so we can compare how different metrics influence which parameter you pick)

# More on calibration - some complications

How to do a maximum likelihood estimate of model results in R

# Glue - generalized uncertainty analysis

What if we wanted to keep all of the 'good' parameters

* we could just keep them all as equally likely
* we could weight them by performance

Either way we can graph and come up with 'best' prediction accounting for uncertainty

Create a single measure of accuracy - above we used *compute_lowlowmetrics_all* to compute an accuracy measure based on

* relative error in annual minimum flow estimate
* relative error in monthly flow during low flow period
* correlation between observed and modelled annual minimum flow
* correlation between observed and modelled flow during the low flow period

We weighted all 4 the same

# Use the accuracy measure 

We can use the combined accuracy measure to define behavioural (acceptable) parameter set (**res_acc**) - two options

* define a threshold for acceptability (we will use 30%)
* take top 50 performing parameter sets

(we go with the latter but code could be commented to go with threshold approach)

```{r behavioral, echo=FALSE}

summary(res$combined)

# 1) selecting behaviorial or acceptable parameters sets

threshold <- 0.3
res_acc <- subset(res, combined > threshold)
head(res_acc)

# as an alternative  what if you want the top N parameter sets
topN = 50
tmp = res[order(res$combined,decreasing=T),]  # reorder from highest to lowest 
res_acc=tmp[1:topN,] # select the highest 50 
head(res_acc)

```

# Defining weights (likelihood) for parameter sets

Now define "weights" (likelihood) based on parameter performance for the acceptable or behaviorial parameters

We want the sum of the weights to equal 1

* accuracy measure defined above will define weight
* we divide by the sum of all accuracy measures to get fractions that add to 1
* note we now only work with behavioural parameter sets (in ** res_acc ** versus ** res **)

```{r weighting, echo=FALSE}

# create a weight for each parameter set based on its relative accuracy - we do this so all weights sum to 1
sum_acc=sum(res_acc$combined)
res_acc$wt_acc=res_acc$combined/sum_acc

head(res_acc)

# look at values
summary(res_acc$wt_acc)

# check to see that they sum to one
sum(res_acc$wt_acc)

Nacc = nrow(res_acc)
Nacc
```

# Using weights

One way to use weights is to define a maximum likelihood estimate by averaging (weighted by accuracy) streamflow from all behavioural simulations 




# generate a streamflow as weighted average of all  acceptable parameter sets

# recall that msagel is the flow data for all runs so we 
# can link with weights from res_acc by run id

# Final step of GLUE

We could also compute quantiles rather than just mean

We won't do this but just in case

We can use the `wtd.quantile()` function in the `reldist` package to do this - it computes quantiles accounting for different weights on each observation

```{r plotting, echo=TRUE}
 
# compute quantiles based on performance weights
quant_flow = msagel_acc %>% group_by(date) %>% dplyr::summarize(
  flow10=wtd.quantile(x=flow, weight=wt_acc, q=0.1),
  flow50=wtd.quantile(x=flow, weight=wt_acc, q=0.5),
  flow90=wtd.quantile(x=flow, weight=wt_acc, q=0.9)
)


# ad observed back
quant_flow_obs = left_join(quant_flow, msage[,c("date","month","year", "day","wy","obs")], 
                       by=c("date"))

# format for plotting
quant_flowl = quant_flow_obs %>% pivot_longer(col=c(flow10, flow50, flow90, obs), 
                                          values_to="flow", names_to="quantile")

# plot
ggplot(subset(quant_flowl, wy==1985), aes(date, flow, col=quantile))+geom_line()

# to see low flows, transform y-axis
ggplot(subset(quant_flowl, wy==1980), aes(date, flow, col=quantile))+
  geom_line()+scale_y_continuous(trans="log")


```

# Rubric 60 pts 

* R function (10pts) 
  * combines at least 2 performance metrics (5)
  * function is applied to part of Sagehen data set (5)
  
* Calibration (20pts)
  * your metrics are used to select 'acceptable' parameter set outcomes (5)
  * metrics are computed for post-calibration data of accepted parameter set outcomes (5)
  * maximum likelihood estimate is computed for post-calibration data (10)
  
* Graphs (20pts)
  * 1-2 plots of summary of performance over calibration period (5) 
  * 1-2 plots of output of acceptable parameter sets that clearly visualize uncertainty (5)
  * plot maximum likelihood estimate for post-calibration period (5) 
  * graphing style (axis labels, legibility) (5)
  
* Discussion (10pts)
  * short explanation on metrics used (5) 
  * 1-2 sentences on calibration and post-calibration uncertainty analysis 



